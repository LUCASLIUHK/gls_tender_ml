{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download all tender results files\n",
    "[HTML elements](https://blog.hubspot.com/website/html-elements)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 107.0.5304\n",
      "Get LATEST chromedriver version for 107.0.5304 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/107.0.5304.62/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\13051\\.wdm\\drivers\\chromedriver\\win32\\107.0.5304.62]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f67dadf3d2d6469a88c169d173037f74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bedok South Avenue 1: Searching...\n",
      "Bedok South Avenue 1: 1 URA sales site searched\n",
      "Bedok South Avenue 1: Extracting URL...\n",
      "Invalid URL\n",
      "Bedok South Avenue 1: 0 valid URLs retrieved\n",
      "Bedok South Avenue 1: Process ended\n",
      "Commonwealth Avenue West / Faber Heights: Searching...\n",
      "Commonwealth Avenue West / Faber Heights: 1 URA sales site searched\n",
      "Commonwealth Avenue West / Faber Heights: Extracting URL...\n",
      "Invalid URL\n",
      "Commonwealth Avenue West / Faber Heights: 0 valid URLs retrieved\n",
      "Commonwealth Avenue West / Faber Heights: Process ended\n",
      "Hillview Avenue / Bukit Batok Town Park: Searching...\n",
      "Hillview Avenue / Bukit Batok Town Park: 1 URA sales site searched\n",
      "Hillview Avenue / Bukit Batok Town Park: Extracting URL...\n",
      "Invalid URL\n",
      "Hillview Avenue / Bukit Batok Town Park: 0 valid URLs retrieved\n",
      "Hillview Avenue / Bukit Batok Town Park: Process ended\n",
      "Bencoolen Street / Albert Street: Searching...\n",
      "Bencoolen Street / Albert Street: 2 URA sales sites searched\n",
      "Bencoolen Street / Albert Street: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Bencoolen Street / Albert Street: Searching...\n",
      "Bencoolen Street / Albert Street: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Bencoolen Street / Albert Street: 0 valid URLs retrieved\n",
      "Bencoolen Street / Albert Street: Process ended\n",
      "Middle Road / Prinsep Sreet: Searching...\n",
      "Middle Road / Prinsep Sreet: 1 URA sales site searched\n",
      "Middle Road / Prinsep Sreet: Extracting URL...\n",
      "Invalid URL\n",
      "Middle Road / Prinsep Sreet: 0 valid URLs retrieved\n",
      "Middle Road / Prinsep Sreet: Process ended\n",
      "Telok Ayer Street / Cross Street: Searching...\n",
      "Telok Ayer Street / Cross Street: 1 URA sales site searched\n",
      "Telok Ayer Street / Cross Street: Extracting URL...\n",
      "Invalid URL\n",
      "Telok Ayer Street / Cross Street: 0 valid URLs retrieved\n",
      "Telok Ayer Street / Cross Street: Process ended\n",
      "Cecil Street / Church Street: Searching...\n",
      "Cecil Street / Church Street: 1 URA sales site searched\n",
      "Cecil Street / Church Street: Extracting URL...\n",
      "Invalid URL\n",
      "Cecil Street / Church Street: 0 valid URLs retrieved\n",
      "Cecil Street / Church Street: Process ended\n",
      "Church Street / Telok Ayer Street: Searching...\n",
      "Church Street / Telok Ayer Street: 1 URA sales site searched\n",
      "Church Street / Telok Ayer Street: Extracting URL...\n",
      "Invalid URL\n",
      "Church Street / Telok Ayer Street: 0 valid URLs retrieved\n",
      "Church Street / Telok Ayer Street: Process ended\n",
      "Penang Road / Oxley Road: Searching...\n",
      "Penang Road / Oxley Road: 2 URA sales sites searched\n",
      "Penang Road / Oxley Road: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Penang Road / Oxley Road: Searching...\n",
      "Penang Road / Oxley Road: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Penang Road / Oxley Road: 0 valid URLs retrieved\n",
      "Penang Road / Oxley Road: Process ended\n",
      "Kaki Bukit Avenue 1 / Kaki Bukit Road 3: Searching...\n",
      "Kaki Bukit Avenue 1 / Kaki Bukit Road 3: 1 URA sales site searched\n",
      "Kaki Bukit Avenue 1 / Kaki Bukit Road 3: Extracting URL...\n",
      "Invalid URL\n",
      "Kaki Bukit Avenue 1 / Kaki Bukit Road 3: 0 valid URLs retrieved\n",
      "Kaki Bukit Avenue 1 / Kaki Bukit Road 3: Process ended\n",
      "Alkaff Quay: Searching...\n",
      "Alkaff Quay: 1 URA sales site searched\n",
      "Alkaff Quay: Extracting URL...\n",
      "Invalid URL\n",
      "Alkaff Quay: 0 valid URLs retrieved\n",
      "Alkaff Quay: Process ended\n",
      "Eastwood Park Phase 2 (Land Parcel F6): Searching...\n",
      "Eastwood Park Phase 2 (Land Parcel F6): 1 URA sales site searched\n",
      "Eastwood Park Phase 2 (Land Parcel F6): Extracting URL...\n",
      "Invalid URL\n",
      "Eastwood Park Phase 2 (Land Parcel F6): 0 valid URLs retrieved\n",
      "Eastwood Park Phase 2 (Land Parcel F6): Process ended\n",
      "Eastwood Park Phase 2 (Land Parcel F7): Searching...\n",
      "Eastwood Park Phase 2 (Land Parcel F7): 1 URA sales site searched\n",
      "Eastwood Park Phase 2 (Land Parcel F7): Extracting URL...\n",
      "Invalid URL\n",
      "Eastwood Park Phase 2 (Land Parcel F7): 0 valid URLs retrieved\n",
      "Eastwood Park Phase 2 (Land Parcel F7): Process ended\n",
      "Eastwood Park Phase 2 (Land Parcel F8): Searching...\n",
      "Eastwood Park Phase 2 (Land Parcel F8): 1 URA sales site searched\n",
      "Eastwood Park Phase 2 (Land Parcel F8): Extracting URL...\n",
      "Invalid URL\n",
      "Eastwood Park Phase 2 (Land Parcel F8): 0 valid URLs retrieved\n",
      "Eastwood Park Phase 2 (Land Parcel F8): Process ended\n",
      "Eastwood Park Phase 2 (Land Parcel G): Searching...\n",
      "Eastwood Park Phase 2 (Land Parcel G): 1 URA sales site searched\n",
      "Eastwood Park Phase 2 (Land Parcel G): Extracting URL...\n",
      "Invalid URL\n",
      "Eastwood Park Phase 2 (Land Parcel G): 0 valid URLs retrieved\n",
      "Eastwood Park Phase 2 (Land Parcel G): Process ended\n",
      "Duchess Avenue: Searching...\n",
      "Duchess Avenue: 2 URA sales sites searched\n",
      "Duchess Avenue: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Duchess Avenue: Searching...\n",
      "Duchess Avenue: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Duchess Avenue: 0 valid URLs retrieved\n",
      "Duchess Avenue: Process ended\n",
      "Sunrise Avenue: Searching...\n",
      "Sunrise Avenue: 2 URA sales sites searched\n",
      "Sunrise Avenue: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Sunrise Avenue: Searching...\n",
      "Sunrise Avenue: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Sunrise Avenue: 0 valid URLs retrieved\n",
      "Sunrise Avenue: Process ended\n",
      "Upper Changi Road East: Searching...\n",
      "Upper Changi Road East: 5 URA sales sites searched\n",
      "Upper Changi Road East: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Upper Changi Road East: Searching...\n",
      "Upper Changi Road East: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Upper Changi Road East: Searching...\n",
      "Upper Changi Road East: Extracting #3 URL...\n",
      "Invalid URL\n",
      "Upper Changi Road East: Searching...\n",
      "Upper Changi Road East: Extracting #4 URL...\n",
      "Invalid URL\n",
      "Upper Changi Road East: Searching...\n",
      "Upper Changi Road East: Extracting #5 URL...\n",
      "Invalid URL\n",
      "Upper Changi Road East: 0 valid URLs retrieved\n",
      "Upper Changi Road East: Process ended\n",
      "Eastwood Park Phase 1 (Land Parcel F1): Searching...\n",
      "Eastwood Park Phase 1 (Land Parcel F1): 1 URA sales site searched\n",
      "Eastwood Park Phase 1 (Land Parcel F1): Extracting URL...\n",
      "Invalid URL\n",
      "Eastwood Park Phase 1 (Land Parcel F1): 0 valid URLs retrieved\n",
      "Eastwood Park Phase 1 (Land Parcel F1): Process ended\n",
      "Eastwood Park Phase 1 (Land Parcel F2): Searching...\n",
      "Eastwood Park Phase 1 (Land Parcel F2): 1 URA sales site searched\n",
      "Eastwood Park Phase 1 (Land Parcel F2): Extracting URL...\n",
      "Invalid URL\n",
      "Eastwood Park Phase 1 (Land Parcel F2): 0 valid URLs retrieved\n",
      "Eastwood Park Phase 1 (Land Parcel F2): Process ended\n",
      "Cross Street / China Street: Searching...\n",
      "Cross Street / China Street: 1 URA sales site searched\n",
      "Cross Street / China Street: Extracting URL...\n",
      "Invalid URL\n",
      "Cross Street / China Street: 0 valid URLs retrieved\n",
      "Cross Street / China Street: Process ended\n",
      "Telok Ayer Street: Searching...\n",
      "Telok Ayer Street: 4 URA sales sites searched\n",
      "Telok Ayer Street: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Telok Ayer Street: Searching...\n",
      "Telok Ayer Street: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Telok Ayer Street: Searching...\n",
      "Telok Ayer Street: Extracting #3 URL...\n",
      "Invalid URL\n",
      "Telok Ayer Street: Searching...\n",
      "Telok Ayer Street: Extracting #4 URL...\n",
      "Done\n",
      "Telok Ayer Street: 1 valid URLs retrieved\n",
      "Telok Ayer Street_0: Tender details saved in <Telok Ayer Street_19950327.pdf>\n",
      "Telok Ayer Street: Process ended\n",
      "Havelock Road / Solomon Street: Searching...\n",
      "Havelock Road / Solomon Street: 1 URA sales site searched\n",
      "Havelock Road / Solomon Street: Extracting URL...\n",
      "Invalid URL\n",
      "Havelock Road / Solomon Street: 0 valid URLs retrieved\n",
      "Havelock Road / Solomon Street: Process ended\n",
      "Jalan Hajijah: Searching...\n",
      "Jalan Hajijah: 1 URA sales site searched\n",
      "Jalan Hajijah: Extracting URL...\n",
      "Invalid URL\n",
      "Jalan Hajijah: 0 valid URLs retrieved\n",
      "Jalan Hajijah: Process ended\n",
      "Jalan Sempadan: Searching...\n",
      "Jalan Sempadan: 1 URA sales site searched\n",
      "Jalan Sempadan: Extracting URL...\n",
      "Invalid URL\n",
      "Jalan Sempadan: 0 valid URLs retrieved\n",
      "Jalan Sempadan: Process ended\n",
      "Lorong 42 Geylang: Searching...\n",
      "Lorong 42 Geylang: 1 URA sales site searched\n",
      "Lorong 42 Geylang: Extracting URL...\n",
      "Invalid URL\n",
      "Lorong 42 Geylang: 0 valid URLs retrieved\n",
      "Lorong 42 Geylang: Process ended\n",
      "Kaki Bukit Avenue 2 / Kaki Bukit Road 1: Searching...\n",
      "Kaki Bukit Avenue 2 / Kaki Bukit Road 1: 1 URA sales site searched\n",
      "Kaki Bukit Avenue 2 / Kaki Bukit Road 1: Extracting URL...\n",
      "Invalid URL\n",
      "Kaki Bukit Avenue 2 / Kaki Bukit Road 1: 0 valid URLs retrieved\n",
      "Kaki Bukit Avenue 2 / Kaki Bukit Road 1: Process ended\n",
      "Woodlands Industrial Park E1: Searching...\n",
      "Woodlands Industrial Park E1: 1 URA sales site searched\n",
      "Woodlands Industrial Park E1: Extracting URL...\n",
      "Invalid URL\n",
      "Woodlands Industrial Park E1: 0 valid URLs retrieved\n",
      "Woodlands Industrial Park E1: Process ended\n",
      "Punggol: Searching...\n",
      "Punggol: 3 URA sales sites searched\n",
      "Punggol: Extracting #1 URL...\n",
      "Done\n",
      "Punggol: Searching...\n",
      "Punggol: Extracting #2 URL...\n",
      "Done\n",
      "Punggol: Searching...\n",
      "Punggol: Extracting #3 URL...\n",
      "Invalid URL\n",
      "Punggol: 2 valid URLs retrieved\n",
      "Punggol_0: Tender details saved in <Punggol Point_20130307_0.pdf>\n",
      "Punggol_1: Tender details saved in <Punggol Central + Punggol Walk_20101210_0.pdf>\n",
      "Punggol: Process ended\n",
      "Bras Basah Road: Searching...\n",
      "Bras Basah Road: 2 URA sales sites searched\n",
      "Bras Basah Road: Extracting #1 URL...\n",
      "Done\n",
      "Bras Basah Road: Searching...\n",
      "Bras Basah Road: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Bras Basah Road: 1 valid URLs retrieved\n",
      "Bras Basah Road_0: Tender details saved in <North Bridge Road + Bras Basah Road_20050127_0.pdf>\n",
      "Bras Basah Road: Process ended\n",
      "Stamford Road: Searching...\n",
      "Stamford Road: 2 URA sales sites searched\n",
      "Stamford Road: Extracting #1 URL...\n",
      "Done\n",
      "Stamford Road: Searching...\n",
      "Stamford Road: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Stamford Road: 1 valid URLs retrieved\n",
      "Stamford Road_0: Tender details saved in <Stamford Road + North Bridge Road_20101027_0.pdf>\n",
      "Stamford Road: Process ended\n",
      "Merchant Road / Boat Quay: Searching...\n",
      "Merchant Road / Boat Quay: 1 URA sales site searched\n",
      "Merchant Road / Boat Quay: Extracting URL...\n",
      "Invalid URL\n",
      "Merchant Road / Boat Quay: 0 valid URLs retrieved\n",
      "Merchant Road / Boat Quay: Process ended\n",
      "Robertson Quay / Nanson Road: Searching...\n",
      "Robertson Quay / Nanson Road: 1 URA sales site searched\n",
      "Robertson Quay / Nanson Road: Extracting URL...\n",
      "Invalid URL\n",
      "Robertson Quay / Nanson Road: 0 valid URLs retrieved\n",
      "Robertson Quay / Nanson Road: Process ended\n",
      "Hill Street / Coleman Street: Searching...\n",
      "Hill Street / Coleman Street: 1 URA sales site searched\n",
      "Hill Street / Coleman Street: Extracting URL...\n",
      "Invalid URL\n",
      "Hill Street / Coleman Street: 0 valid URLs retrieved\n",
      "Hill Street / Coleman Street: Process ended\n",
      "Tanglin Road: Searching...\n",
      "Tanglin Road: 1 URA sales site searched\n",
      "Tanglin Road: Extracting URL...\n",
      "Invalid URL\n",
      "Tanglin Road: 0 valid URLs retrieved\n",
      "Tanglin Road: Process ended\n",
      "Bayshore Road: Searching...\n",
      "Bayshore Road: 2 URA sales sites searched\n",
      "Bayshore Road: Extracting #1 URL...\n",
      "Invalid URL\n",
      "Bayshore Road: Searching...\n",
      "Bayshore Road: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Bayshore Road: 0 valid URLs retrieved\n",
      "Bayshore Road: Process ended\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: 14 URA sales sites searched\n",
      "Kaki Bukit: Extracting #1 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #2 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #3 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #4 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #5 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #6 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #7 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #8 URL...\n",
      "Done\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #9 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #10 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #11 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #12 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #13 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: Searching...\n",
      "Kaki Bukit: Extracting #14 URL...\n",
      "Invalid URL\n",
      "Kaki Bukit: 7 valid URLs retrieved\n",
      "Kaki Bukit_0: Tender details saved in <Kaki Bukit Road 4_20111027_0.pdf>\n",
      "Kaki Bukit_1: Tender details saved in <Kaki Bukit Road 5 + Kaki Bukit Avenue 6_20120619_0.pdf>\n",
      "Kaki Bukit_2: Tender details saved in <Kaki Bukit Road 2_20090817_0.pdf>\n",
      "Kaki Bukit_3: Tender details saved in <Kaki Bukit Road 4_20110302_0.pdf>\n",
      "Kaki Bukit_4: Tender details saved in <Kaki Bukit Avenue 4_20100917_0.pdf>\n",
      "Kaki Bukit_5: Tender details saved in <Kaki Bukit Road 2_20090504_0.pdf>\n",
      "Kaki Bukit_6: Tender details saved in <Kaki Bukit Road 2_20080626.pdf>\n",
      "Kaki Bukit: Process ended\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: Searching...\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: 2 URA sales sites searched\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: Extracting #1 URL...\n",
      "Done\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: Searching...\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: Extracting #2 URL...\n",
      "Invalid URL\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: 1 valid URLs retrieved\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4_0: Tender details saved in <Tuas South Avenue 3 + Tuas South Avenue 4_20000711.pdf>\n",
      "Tuas South Avenue 3 / Tuas South Avenue 4: Process ended\n",
      "Tuas South Avenue 4 / Tuas South Avenue 5: Searching...\n",
      "Tuas South Avenue 4 / Tuas South Avenue 5: 1 URA sales site searched\n",
      "Tuas South Avenue 4 / Tuas South Avenue 5: Extracting URL...\n",
      "Invalid URL\n",
      "Tuas South Avenue 4 / Tuas South Avenue 5: 0 valid URLs retrieved\n",
      "Tuas South Avenue 4 / Tuas South Avenue 5: Process ended\n",
      "Merchant Road / Angus Street: Searching...\n",
      "Merchant Road / Angus Street: 1 URA sales site searched\n",
      "Merchant Road / Angus Street: Extracting URL...\n",
      "Invalid URL\n",
      "Merchant Road / Angus Street: 0 valid URLs retrieved\n",
      "Merchant Road / Angus Street: Process ended\n",
      "Merchant Road / Magazine Road: Searching...\n",
      "Merchant Road / Magazine Road: 1 URA sales site searched\n",
      "Merchant Road / Magazine Road: Extracting URL...\n",
      "Invalid URL\n",
      "Merchant Road / Magazine Road: 0 valid URLs retrieved\n",
      "Merchant Road / Magazine Road: Process ended\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "# from fake_useragent import UserAgent\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "from random import choice\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import urllib3\n",
    "import warnings\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# os.chdir(r'G:\\\\REA\\\\Working files\\\\land-bidding\\\\Table extraction')\n",
    "with open('user_agent.txt') as f:\n",
    "    ua_list = [ua.strip() for ua in f]\n",
    "    f.close()\n",
    "\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.DEBUG)\n",
    "handler = logging.FileHandler(filename='scraping.log', mode='w', encoding='utf-8')\n",
    "formatter = logging.Formatter(\"%(asctime)s %(name)s:%(levelname)s:%(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "root_logger.addHandler(handler)\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "original_wd = os.getcwd()\n",
    "\n",
    "\n",
    "def random_time_delay():\n",
    "    time.sleep(random.uniform(15, 30))\n",
    "\n",
    "\n",
    "class WebScraper:\n",
    "\n",
    "    def __init__(self, save_path: str = None, logger=None):\n",
    "        self.save_path = save_path\n",
    "        self.driver = self.open_browser()\n",
    "        self.logger = logger\n",
    "\n",
    "    def open_browser(self):\n",
    "        chrome_options = uc.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument('--disable-notifications')\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--ignore-certificate-errors')\n",
    "        chrome_options.add_argument(f\"user-agent={choice(ua_list)}\")\n",
    "        chrome_options.add_experimental_option('prefs',\n",
    "                                               {\"download.default_directory\": self.save_path,\n",
    "                                                \"download.prompt_for_download\": False,\n",
    "                                                \"download.directory_upgrade\": True,\n",
    "                                                \"plugins.always_open_pdf_externally\": True\n",
    "                                                }\n",
    "                                               )\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "        return driver\n",
    "\n",
    "    def log(self, msg: str, level='info'):\n",
    "        print(msg)\n",
    "        if self.logger:\n",
    "            try:\n",
    "                if level == 'warning':\n",
    "                    self.logger.warning(msg)\n",
    "                elif level == 'error':\n",
    "                    self.logger.error(msg)\n",
    "                else:\n",
    "                    self.logger.info(msg)\n",
    "            except:\n",
    "                pass\n",
    "        return msg\n",
    "\n",
    "    def search(self, land_parcel: str):\n",
    "        self.log(f'{self.land_parcel}: Searching...')\n",
    "        try:\n",
    "            # go to home page of ura gov site\n",
    "            self.driver.get('https://www.ura.gov.sg/maps/')\n",
    "            random_time_delay()\n",
    "\n",
    "            # click on \"view government land sales site\"\n",
    "            self.driver.find_element(by=By.XPATH,\n",
    "                                     value='//*[@id=\"us-c-ip\"]/div[1]/div[1]/div[4]/div[3]/div[6]/div[2]').click()\n",
    "            random_time_delay()\n",
    "\n",
    "            # search land parcel\n",
    "            self.driver.find_element(by=By.CLASS_NAME, value='us-s-txt').send_keys(land_parcel)\n",
    "\n",
    "        except:\n",
    "            self.log(f'{land_parcel}: Error occurred when searching', 'error')\n",
    "\n",
    "    def extract_url(self):\n",
    "        source = self.driver.page_source\n",
    "        soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "        url = None\n",
    "\n",
    "        for i in soup.find_all('a'):\n",
    "            try:\n",
    "                if ('Tender-Results' in i['href']):\n",
    "                    url = i['href']\n",
    "                    random_time_delay()\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return url\n",
    "\n",
    "    def get_url(self, land_parcel: str) -> [List[str], List[str], List[str]]:\n",
    "        self.land_parcel = land_parcel\n",
    "\n",
    "        try:\n",
    "            self.search(land_parcel)\n",
    "            random_time_delay()\n",
    "\n",
    "            # get the number of searched results\n",
    "            results = self.driver.find_elements(by=By.XPATH, value='//a[@data-parentid=\"0\" and @data-type=\"service\"]')\n",
    "\n",
    "            url_list = []\n",
    "            awardDates = []\n",
    "            nameList = []\n",
    "            if len(results) == 0:\n",
    "                self.log(f\"{self.land_parcel}: No URA sales site result\", \"warning\")\n",
    "\n",
    "            elif len(results) == 1:\n",
    "                self.log(f\"{self.land_parcel}: 1 URA sales site searched\")\n",
    "                # Click on result\n",
    "                random_time_delay()\n",
    "                self.driver.find_element(by=By.CLASS_NAME, value='us-sr-result').click()\n",
    "\n",
    "                # Get url for Tender Result pdf\n",
    "                self.log(f\"{self.land_parcel}: Extracting URL...\")\n",
    "                random_time_delay()\n",
    "                url = self.extract_url()\n",
    "                if url and pd.notna(url):\n",
    "                    url_list.append(url)\n",
    "                    self.log(\"Done\")\n",
    "                else:\n",
    "                    self.log(\"Invalid URL\", \"warning\")\n",
    "                random_time_delay()\n",
    "\n",
    "                # get the exact land parcel name and the dates of award\n",
    "                try:\n",
    "                    land_parcel_name = self.driver.find_element(by=By.CLASS_NAME, value=\"us-ip-poi-a-title\").text\n",
    "                    random_time_delay()\n",
    "                except:\n",
    "                    land_parcel_name = self.land_parcel\n",
    "\n",
    "                try:\n",
    "                    award_date = self.driver.find_element(by=By.XPATH,\n",
    "                                                          value='//*[@id=\"us-c-ip\"]/div[3]/div[1]/div[2]/div[1]/div[2]/div[1]/div[2]/div[4]/div[2]').text\n",
    "                    award_date = datetime.strptime(award_date, '%d %B %Y').strftime(\"%Y%m%d\")\n",
    "                except:\n",
    "                    award_date = 'Unknown'\n",
    "\n",
    "                awardDates.append(award_date)\n",
    "                nameList.append(land_parcel_name)\n",
    "\n",
    "\n",
    "            else:\n",
    "                self.log(f\"{self.land_parcel}: {len(results)} URA sales sites searched\")\n",
    "                for id_ in range(len(results)):  # 0 1, len = 2\n",
    "                    self.log(f\"{self.land_parcel}: Extracting #{id_ + 1} URL...\")\n",
    "\n",
    "                    # click one by one\n",
    "                    random_time_delay()\n",
    "                    self.driver.find_element(by=By.XPATH,\n",
    "                                             value=f'//a[@data-type=\"service\" and @data-id=\"{id_}\"]').click()\n",
    "\n",
    "                    # Get url for Tender Result pdf\n",
    "                    random_time_delay()\n",
    "                    url = self.extract_url()\n",
    "                    if url and pd.notna(url):\n",
    "                        url_list.append(url)\n",
    "                        self.log(\"Done\")\n",
    "                    else:\n",
    "                        self.log(\"Invalid URL\", \"warning\")\n",
    "                    random_time_delay()\n",
    "\n",
    "                    # get the exact land parcel name and the dates of award\n",
    "                    try:\n",
    "                        land_parcel_name = self.driver.find_element(by=By.CLASS_NAME, value=\"us-ip-poi-a-title\").text\n",
    "                        random_time_delay()\n",
    "                    except:\n",
    "                        land_parcel_name = self.land_parcel\n",
    "\n",
    "                    try:\n",
    "                        award_date = self.driver.find_element(by=By.XPATH,\n",
    "                                                              value='//*[@id=\"us-c-ip\"]/div[3]/div[1]/div[2]/div[1]/div[2]/div[1]/div[2]/div[4]/div[2]').text\n",
    "                        award_date = datetime.strptime(award_date, '%d %B %Y').strftime(\"%Y%m%d\")\n",
    "                    except:\n",
    "                        award_date = 'Unknown'\n",
    "\n",
    "                    awardDates.append(award_date)\n",
    "                    nameList.append(land_parcel_name)\n",
    "\n",
    "                    # get back to the search results\n",
    "                    if id_ < len(results) - 1:  # 1\n",
    "                        random_time_delay()\n",
    "                        self.search(land_parcel)\n",
    "\n",
    "            self.log(f\"{self.land_parcel}: {len(url_list)} valid URLs retrieved\")\n",
    "            return [url_list, awardDates, nameList]\n",
    "\n",
    "        except:\n",
    "            self.log(f\"{self.land_parcel}: Error occurred when getting URL list\", \"error\")\n",
    "            pass\n",
    "\n",
    "    def download(self, url_list: List[str], id_list: List[str] = None, name_list: List[str] = None):\n",
    "        if not os.path.exists(os.path.join(self.save_path, 'temp')):\n",
    "            os.makedirs(os.path.join(self.save_path, 'temp'))\n",
    "        destination = os.path.join(self.save_path, 'temp')\n",
    "        if not id_list:\n",
    "            id_list = list(range(len(url_list)))\n",
    "\n",
    "        if not name_list:\n",
    "            name_list = [self.land_parcel]*len(url_list)\n",
    "\n",
    "        for i in range(len(url_list)):\n",
    "            _id_ = id_list[i]\n",
    "            _exactName_ = name_list[i]\n",
    "\n",
    "            # make sure the save path has no pdf file (just serve as a mid-point), if not, move these files to another folder\n",
    "            existing_pdf = [file for file in os.listdir(self.save_path) if '.pdf' in file]\n",
    "            if len(existing_pdf) > 0:\n",
    "                if not os.path.exists(os.path.join(self.save_path, 'redundant')):\n",
    "                    os.makedirs(os.path.join(self.save_path, 'redundant'))\n",
    "                redundant_path = os.path.join(self.save_path, 'redundant')\n",
    "\n",
    "                for file in existing_pdf:\n",
    "                    source_redundant = os.path.join(self.save_path, file)\n",
    "                    desti_redundant = os.path.join(redundant_path, file)\n",
    "                    shutil.move(source_redundant, desti_redundant)\n",
    "\n",
    "            random_time_delay()\n",
    "            try:\n",
    "                self.driver.get(url_list[i])\n",
    "\n",
    "                random_time_delay()\n",
    "                pdf = [file for file in os.listdir(self.save_path) if '.pdf' in file]\n",
    "                if len(pdf) > 0:\n",
    "                    pdf_file = pdf[0]\n",
    "                    if len(pdf) > 1:\n",
    "                        self.log(f\"{self.land_parcel}_{i}: Multiple PDF downloaded or redundant files, chose the first one\", \"warning\")\n",
    "                    source_path = os.path.join(self.save_path, pdf_file)\n",
    "                    # remove illegal punc in filename\n",
    "                    illegal_punc = '[/\\:*?\"<>|]'\n",
    "\n",
    "                    try:\n",
    "                        file_name = re.sub(illegal_punc, '+', _exactName_)\n",
    "                    except:\n",
    "                        file_name = re.sub(illegal_punc, '+', self.land_parcel)\n",
    "\n",
    "                    full_file_name = f\"{file_name}_{_id_}.pdf\"\n",
    "\n",
    "                    # make sure there's no duplicated file name\n",
    "                    filelist = os.listdir(destination)\n",
    "                    occurrence = filelist.count(full_file_name)\n",
    "                    if occurrence:\n",
    "                        k = 0\n",
    "                        full_file_name = f\"{file_name}_{_id_}_0.pdf\"\n",
    "                        while filelist.count(full_file_name):\n",
    "                            k += 1\n",
    "                            full_file_name = f\"{file_name}_{_id_}_{k}.pdf\"\n",
    "\n",
    "                    desti_path = os.path.join(destination, full_file_name)\n",
    "                    shutil.move(source_path, desti_path)\n",
    "                    self.log(f\"{self.land_parcel}_{i}: Tender details saved in <{full_file_name}>\")\n",
    "\n",
    "                else:\n",
    "                    self.log(f\"{self.land_parcel}_{i}: No PDF downloaded\", \"warning\")\n",
    "            except:\n",
    "                self.log(f\"{self.land_parcel}_{i}: Error occurred when downloading\", \"error\")\n",
    "\n",
    "        self.log(f'{self.land_parcel}: Process ended', '\\n')\n",
    "\n",
    "    def scrape(self, landParcels: List[str]):\n",
    "        try:\n",
    "            for land_parcel in tqdm(landParcels):\n",
    "                [url_list, id_list, name_list] = self.get_url(land_parcel)\n",
    "                self.download(url_list, id_list, name_list)\n",
    "            self.log(\"All done\")\n",
    "            self.driver.quit()\n",
    "        except:\n",
    "            self.log(f'{self.land_parcel}: Error occurred when passing URL to download function', \"error\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read in land parcel list\n",
    "    gls = pd.read_csv(r'G:\\REA\\Working files\\land-bidding\\land_sales_full_data\\ready for uploading\\gls_no_detail.csv')\n",
    "    ura_gls = gls[gls.source == 'ura'].reset_index(drop=True)\n",
    "    landParcels = list(ura_gls.land_parcel.reset_index(drop=True).apply(lambda x: x.replace('/', ' / ')).unique())\n",
    "\n",
    "    # start scraping\n",
    "    save_path = r'G:\\REA\\Working files\\land-bidding\\Table extraction\\tenderer_details_ura'\n",
    "    scraper = WebScraper(save_path=save_path, logger=root_logger)\n",
    "    scraper.scrape(landParcels[287:])\n",
    "    # scraper.scrape(['Chestnut Avenue'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "287"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landParcels.index('Bedok South Avenue 1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc_20000001_2.pdf\n"
     ]
    }
   ],
   "source": [
    "filelist = ['abc_20000001_0.pdf', '21431243', 'asffw.txt', 'abc_20000001_1.pdf', 'abc_20000001.pdf']\n",
    "file_name = 'abc'\n",
    "_id_ = '20000001'\n",
    "full_file_name = f\"{file_name}_{_id_}.pdf\"\n",
    "occurrence = filelist.count(full_file_name)\n",
    "if occurrence:\n",
    "    i = 0\n",
    "    full_file_name = f\"{file_name}_{_id_}_0.pdf\"\n",
    "    while filelist.count(full_file_name):\n",
    "        i += 1\n",
    "        full_file_name = f\"{file_name}_{_id_}_{i}.pdf\"\n",
    "print(full_file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# from fake_useragent import UserAgent\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import undetected_chromedriver as uc\n",
    "from random import choice\n",
    "import time\n",
    "import random\n",
    "import urllib3\n",
    "import warnings\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# os.chdir(r'G:\\\\REA\\\\Working files\\\\land-bidding\\\\Table extraction')\n",
    "with open('user_agent.txt') as f:\n",
    "    ua_list = [ua.strip() for ua in f]\n",
    "    f.close()\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "original_wd = os.getcwd()\n",
    "\n",
    "def random_time_delay():\n",
    "    time.sleep(random.uniform(15, 30))\n",
    "\n",
    "\n",
    "class web_scraper:\n",
    "\n",
    "    def __init__(self, save_path: str=None):\n",
    "        self.save_path = save_path\n",
    "        self.driver = self.open_browser()\n",
    "\n",
    "\n",
    "    def open_browser(self):\n",
    "        chrome_options = uc.ChromeOptions()\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument('--disable-notifications')\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--ignore-certificate-errors')\n",
    "        chrome_options.add_argument(f\"user-agent={choice(ua_list)}\")\n",
    "        chrome_options.add_experimental_option('prefs',\n",
    "                                               {\"download.default_directory\": self.save_path,\n",
    "                                                \"download.prompt_for_download\": False,\n",
    "                                                \"download.directory_upgrade\": True,\n",
    "                                                \"plugins.always_open_pdf_externally\": True\n",
    "                                                }\n",
    "        )\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "        return driver\n",
    "\n",
    "\n",
    "    def search(self, land_parcel: str):\n",
    "        print(f'{self.land_parcel}: Searching...')\n",
    "        try:\n",
    "            # go to home page of ura gov site\n",
    "            self.driver.get('https://www.ura.gov.sg/maps/')\n",
    "            random_time_delay()\n",
    "\n",
    "            # click on \"view government land sales site\"\n",
    "            self.driver.find_element_by_xpath('//*[@id=\"us-c-ip\"]/div[1]/div[1]/div[4]/div[3]/div[6]/div[2]').click()\n",
    "            random_time_delay()\n",
    "\n",
    "            # search land parcel\n",
    "            self.driver.find_element_by_class_name('us-s-txt').send_keys(land_parcel)\n",
    "\n",
    "        except:\n",
    "            print(f'{land_parcel}: Error occurred when searching')\n",
    "\n",
    "\n",
    "    def extract_url(self):\n",
    "        source = self.driver.page_source\n",
    "        soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "        url = None\n",
    "\n",
    "        for i in soup.find_all('a'):\n",
    "            try:\n",
    "                if('Tender-Results' in i['href']):\n",
    "                    url = i['href']\n",
    "                    random_time_delay()\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        return url\n",
    "\n",
    "\n",
    "    def get_url(self, land_parcel: str)-> List[str]:\n",
    "        self.land_parcel = land_parcel\n",
    "\n",
    "        try:\n",
    "            self.search(land_parcel)\n",
    "            random_time_delay()\n",
    "\n",
    "            # get the number of searched results\n",
    "            results = self.driver.find_elements_by_xpath('//a[@data-type=\"service\"]')\n",
    "\n",
    "            url_list = []\n",
    "            if len(results) == 0:\n",
    "                print(f\"{self.land_parcel}: No searched result\")\n",
    "\n",
    "            elif len(results) == 1:\n",
    "                print(f\"{self.land_parcel}: 1 result searched\")\n",
    "                # Click on result\n",
    "                random_time_delay()\n",
    "                self.driver.find_element_by_class_name('us-sr-result').click()\n",
    "\n",
    "                # Get url for Tender Result pdf\n",
    "                print(f\"{self.land_parcel}: Extracting URL...\")\n",
    "                random_time_delay()\n",
    "                url = self.extract_url()\n",
    "                if url and pd.notna(url):\n",
    "                    url_list.append(url)\n",
    "                    print(\"Done\")\n",
    "                else:\n",
    "                    print(\"Invalid URL\")\n",
    "\n",
    "            else:\n",
    "                print(f\"{self.land_parcel}: {len(results)} results searched\")\n",
    "                for id_ in range(len(results)): # 0 1, len = 2\n",
    "                    print(f\"{self.land_parcel}: Extracting #{id_+1} URL...\")\n",
    "\n",
    "                    # click one by one\n",
    "                    random_time_delay()\n",
    "                    self.driver.find_element_by_xpath(f'//a[@data-type=\"service\" and @data-id=\"{id_}\"]').click()\n",
    "\n",
    "                    # Get url for Tender Result pdf\n",
    "                    random_time_delay()\n",
    "                    url = self.extract_url()\n",
    "                    if url and pd.notna(url):\n",
    "                        url_list.append(url)\n",
    "                        print(\"Done\")\n",
    "                    else:\n",
    "                        print(\"Invalid URL\")\n",
    "\n",
    "                    # get back to the search results\n",
    "                    if id_ < len(results)-1: # 1\n",
    "                        random_time_delay()\n",
    "                        self.search(land_parcel)\n",
    "\n",
    "            print(f\"{self.land_parcel}: {len(url_list)} valid URLs retrieved\")\n",
    "            return url_list\n",
    "\n",
    "        except:\n",
    "            print(f\"{self.land_parcel}: Error occurred when getting URL list\")\n",
    "            pass\n",
    "\n",
    "\n",
    "    def download(self, url_list: List[str]):\n",
    "        if os.path.exists(os.path.join(self.save_path, 'temp')) is False:\n",
    "            os.makedirs(os.path.join(self.save_path, 'temp'))\n",
    "        destination = os.path.join(self.save_path, 'temp')\n",
    "\n",
    "        for i in range(len(url_list)):\n",
    "            random_time_delay()\n",
    "            try:\n",
    "                self.driver.get(url_list[i])\n",
    "\n",
    "                random_time_delay()\n",
    "                pdf = [file for file in os.listdir(self.save_path) if '.pdf' in file]\n",
    "                if len(pdf) > 0:\n",
    "                    pdf_file = pdf[0]\n",
    "\n",
    "                    if len(pdf) > 1:\n",
    "                        print(f\"{self.land_parcel}_{i}: Multiple PDF downloaded, chose the first one\")\n",
    "\n",
    "                    source_path = os.path.join(self.save_path, pdf_file)\n",
    "                    desti_path = os.path.join(destination, f\"{self.land_parcel}_{i}.pdf\")\n",
    "                    shutil.move(source_path, desti_path)\n",
    "                    print(f\"{self.land_parcel}_{i}: Tender details saved in <{self.land_parcel}_{i}.pdf>\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"{self.land_parcel}_{i}: No PDF downloaded\")\n",
    "            except:\n",
    "                print(f\"{self.land_parcel}_{i}: Error occurred when downloading\")\n",
    "\n",
    "        print(f'{self.land_parcel}: Process ended', '\\n')\n",
    "\n",
    "\n",
    "# scraper = web_scraper(save_path='G:\\\\REA\\\\Working files\\\\land-bidding\\\\Table extraction\\\\tenderer_details_ura')\n",
    "# land_parcel = 'Lentor Hills Road (Parcel B)'\n",
    "# scraper.download(['https://www.ura.gov.sg/-/media/Corporate/Land-Sales/Tender-Results/2022/pr22-32a.pdf'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 107.0.5304\n",
      "Get LATEST chromedriver version for 107.0.5304 google-chrome\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/107.0.5304.62/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\13051\\.wdm\\drivers\\chromedriver\\win32\\107.0.5304.62]\n"
     ]
    }
   ],
   "source": [
    "save_path = r'G:\\REA\\Working files\\land-bidding\\Table extraction\\tenderer_details_ura'\n",
    "scraper = web_scraper(save_path=save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "land_parcel = 'Clementi Avenue 1'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lentor Central: Searching...\n",
      "Lentor Central: 3 results searched\n",
      "Lentor Central: Extracting 1 URL...\n",
      "Done\n",
      "Lentor Central: Searching...\n",
      "Lentor Central: Extracting 2 URL...\n",
      "Done\n",
      "Lentor Central: Searching...\n",
      "Lentor Central: Extracting 3 URL...\n",
      "Invalid URL\n",
      "Lentor Central: Searching...\n",
      "Lentor Central: 2 valid URLs retrieved\n",
      "Lentor Central_0: Tender details saved in <Lentor Central_0.pdf>\n",
      "Lentor Central_1: Tender details saved in <Lentor Central_1.pdf>\n",
      "Lentor Central: Process ended \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scraper.download(scraper.get_url(land_parcel))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "data": {
      "text/plain": "'G:\\\\REA\\\\Working files\\\\land-bidding\\\\Table extraction\\\\tenderer_details_ura\\\\temp\\\\lentor hills p b_0.pdf'"
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save = scraper.save_path\n",
    "source = os.path.join(save, 'pr22-32a.pdf')\n",
    "destination = os.path.join(save, 'temp')\n",
    "desti = os.path.join(destination, 'lentor hills p b_0.pdf')\n",
    "shutil.move(source, desti)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lentor Hills Road (Parcel B)\n"
     ]
    },
    {
     "data": {
      "text/plain": "'https://www.ura.gov.sg/-/media/Corporate/Land-Sales/Tender-Results/2022/pr22-32a.pdf'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    # driver.find_element_by_class_name('us-sr-title')\n",
    "    results = driver.find_elements_by_xpath('//a[@data-type=\"service\"]')\n",
    "    random_time_delay()\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(f\"No result for {land_parcel}\")\n",
    "    elif len(results) == 1:\n",
    "        print(results[0].text)\n",
    "        # Click on result\n",
    "        driver.find_element_by_class_name('us-sr-result').click()\n",
    "        random_time_delay()\n",
    "\n",
    "        # Get url for Tender Result pdf\n",
    "        source = driver.page_source\n",
    "        soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "        for i in soup.find_all('a'):\n",
    "            try:\n",
    "                if('Tender-Results' in i['href']):\n",
    "                    url = i['href']\n",
    "                    random_time_delay()\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print(f\"Multiple results for {land_parcel}\")\n",
    "        for id in range(len(results)):\n",
    "            driver.find_elements_by_xpath('//a[@data-type=\"service\" and @data-id=\"0\"]')[id].click()\n",
    "\n",
    "\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "url"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath(f'//a[@data-type=\"service\" and @data-id=\"{1}\"]').click()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "'//a[@data-type=\"service\" and @data-id=\"1\"]'"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'//a[@data-type=\"service\" and @data-id=\"{1}\"]'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "driver.quit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download PDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "driver.get('https://www.ura.gov.sg/-/media/Corporate/Land-Sales/Tender-Results/2022/pr22-32a.pdf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 107.0.5304\n",
      "Get LATEST chromedriver version for 107.0.5304 google-chrome\n",
      "Driver [C:\\Users\\13051\\.wdm\\drivers\\chromedriver\\win32\\107.0.5304.62\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "download_dir = r\"G:\\REA\\Working files\\land-bidding\\Table extraction\\tenderer_details_ura\"\n",
    "\n",
    "chrome_options = uc.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "chrome_options.add_argument(\"--mute-audio\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument(f\"user-agent={choice(ua_list)}\")\n",
    "chrome_options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "}\n",
    "                                       )\n",
    "\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install(), options = chrome_options)\n",
    "browser.get('https://www.ura.gov.sg/-/media/Corporate/Land-Sales/Tender-Results/2022/pr22-32a.pdf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 107.0.5304\n",
      "Get LATEST chromedriver version for 107.0.5304 google-chrome\n",
      "Driver [C:\\Users\\13051\\.wdm\\drivers\\chromedriver\\win32\\107.0.5304.62\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "save_path = r\"G:\\REA\\Working files\\land-bidding\\Table extraction\\tenderer_details_ura\"\n",
    "chrome_options = uc.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "chrome_options.add_argument(\"--mute-audio\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument(f\"user-agent={choice(ua_list)}\")\n",
    "chrome_options.add_experimental_option('prefs',\n",
    "                                       {\"download.default_directory\": save_path,\n",
    "                                        \"download.prompt_for_download\": False,\n",
    "                                        \"download.directory_upgrade\": True,\n",
    "                                        \"plugins.always_open_pdf_externally\": True\n",
    "                                        }\n",
    "                                       )\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
    "driver.get('https://www.ura.gov.sg/-/media/Corporate/Land-Sales/Tender-Results/2022/pr22-32a.pdf')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 107.0.5304\n",
      "Get LATEST chromedriver version for 107.0.5304 google-chrome\n",
      "Driver [C:\\Users\\13051\\.wdm\\drivers\\chromedriver\\win32\\107.0.5304.62\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "download_dir = r\"G:\\REA\\Working files\\land-bidding\\Table extraction\\tenderer_details_ura\"\n",
    "\n",
    "chrome_options = uc.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "chrome_options.add_argument(\"--mute-audio\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--ignore-certificate-errors')\n",
    "chrome_options.add_argument(f\"user-agent={choice(ua_list)}\")\n",
    "chrome_options.add_experimental_option('prefs',  {\n",
    "    \"download.default_directory\": download_dir,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True\n",
    "}\n",
    "                                       )\n",
    "\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install(), options = chrome_options)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "from random import choice\n",
    "import time\n",
    "import random\n",
    "import urllib3\n",
    "import warnings\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "from typing import List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "browser.get('https://www.ura.gov.sg/maps/')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "browser.find_element_by_xpath('//*[@id=\"us-c-ip\"]/div[1]/div[1]/div[4]/div[3]/div[6]/div[2]').click()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "browser.find_element_by_class_name('us-s-txt').send_keys('Chestnut Avenue')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "results = browser.find_elements(by=By.XPATH, value='//a[@data-parentid=\"0\" and @data-type=\"service\"]')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "browser.find_element(by=By.XPATH, value=f'//a[@data-parentid=\"0\" and@data-type=\"service\" and @data-id=\"0\"]').click()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'Chestnut Avenue'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browser.find_element(by=By.CLASS_NAME, value=\"us-ip-poi-a-title\").text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "date = browser.find_element(by=By.XPATH, value='//*[@id=\"us-c-ip\"]/div[3]/div[1]/div[2]/div[1]/div[2]/div[1]/div[2]/div[4]/div[2]').text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 107.0.5304\n",
      "Get LATEST chromedriver version for 107.0.5304 google-chrome\n",
      "Driver [C:\\Users\\13051\\.wdm\\drivers\\chromedriver\\win32\\107.0.5304.62\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "scraper1 = WebScraper()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chestnut Avenue: Searching...\n",
      "Chestnut Avenue: Error occurred when searching\n",
      "Chestnut Avenue: Error occurred when getting URL list\n"
     ]
    }
   ],
   "source": [
    "res = scraper.get_url('Chestnut Avenue')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'20000301'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.strptime('1 March 2000', '%d %B %Y').strftime(\"%Y%m%d\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 1, 1]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,1,1]\n",
    "if not l:\n",
    "    l = list(range(3))\n",
    "l"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
